{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubertemporal behavior\n",
    "\n",
    "This chapter demonstrates how a 1D model can be learned over DBFP records.\n",
    "\n",
    "The main goal of this notebook is to train a 1D model per application and per user. `Kuberspatiotemporal` is used to model the time when an authetications was requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ipywidgets import interact, interactive, Layout, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kuberspatiotemporal import CompoundModel, Feature, SpatialModel, KuberModel\n",
    "from kuberspatiotemporal.tools import make_ellipses\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"/Users/adrianai/Desktop/aiml/aiml-dbfp-anomaly/utils/user_dbfp_mfa.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 # authentications per application per user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning behaviors (see respective chapter), it is interesting to sort by applications and see how many datapoints per user and per app (to count combinations, see [here](https://stackoverflow.com/a/25259333)) are in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 10\n",
    "df_per_app_per_user_approved = (\n",
    "    data[data[\"mfa_status\"] == \"approved\"]\n",
    "    .groupby([\"user_id\", \"organization_id\", \"oauth_application_id\"], as_index=False)[\n",
    "        \"created_at\"\n",
    "    ]\n",
    "    .count()\n",
    "    .rename(columns={\"created_at\": \"approved\"})\n",
    "    .sort_values(by=\"approved\", ascending=False)\n",
    ")\n",
    "df_per_app_per_user_rejected = (\n",
    "    data[data[\"mfa_status\"] == \"rejected\"]\n",
    "    .groupby([\"user_id\", \"organization_id\", \"oauth_application_id\"], as_index=False)[\n",
    "        \"created_at\"\n",
    "    ]\n",
    "    .count()\n",
    "    .rename(columns={\"created_at\": \"rejected\"})\n",
    "    .sort_values(by=\"rejected\", ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "df_per_app_per_user = pd.merge(\n",
    "    df_per_app_per_user_approved,\n",
    "    df_per_app_per_user_rejected,\n",
    "    on=[\"user_id\", \"organization_id\", \"oauth_application_id\"],\n",
    ")\n",
    "df_per_app_per_user = df_per_app_per_user.sort_values(\n",
    "    by=[\"approved\", \"rejected\"], ascending=False\n",
    ")\n",
    "\n",
    "\n",
    "def show_dbfp_app_user_counts(i):\n",
    "    display(df_per_app_per_user.iloc[i * step : i * step + step])\n",
    "\n",
    "\n",
    "show_app_user_counts = interactive(\n",
    "    show_dbfp_app_user_counts,\n",
    "    i=IntSlider(\n",
    "        min=0,\n",
    "        max=int(df_per_app_per_user.shape[0] / step - 1),\n",
    "        layout=Layout(width=\"30%\", height=\"100px\"),\n",
    "    ),\n",
    ")\n",
    "show_app_user_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_app_per_user[\n",
    "    (df_per_app_per_user[\"approved\"] > 30) & (df_per_app_per_user[\"rejected\"] > 30)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, org, app = (\n",
    "    df_per_app_per_user[\n",
    "        (df_per_app_per_user[\"approved\"] > 30) & (df_per_app_per_user[\"rejected\"] > 30)\n",
    "    ][[\"user_id\", \"organization_id\", \"oauth_application_id\"]]\n",
    "    .head(10)\n",
    "    .values.tolist()[4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, org, app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split test and train samples\n",
    "In point 5 we will calculate the model's performance metrics and, for that, we must divide the data beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_approved = data[\n",
    "    (data[\"mfa_status\"] == \"approved\")\n",
    "    & (data[\"user_id\"] == user)\n",
    "    & (data[\"oauth_application_id\"] == app)\n",
    "    & (data[\"organization_id\"] == org)\n",
    "]\n",
    "data_rejected = data[\n",
    "    (data[\"mfa_status\"] == \"rejected\")\n",
    "    & (data[\"user_id\"] == user)\n",
    "    & (data[\"oauth_application_id\"] == app)\n",
    "    & (data[\"organization_id\"] == org)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_approved = np.random.rand(len(data_approved)) < 0.7\n",
    "msk_rejected = np.random.rand(len(data_rejected)) < 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_approved = data_approved[msk_approved]\n",
    "test_data_approved = data_approved[~msk_approved]\n",
    "\n",
    "train_data_approved.shape[0], test_data_approved.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_approved.shape[0] / (\n",
    "    train_data_approved.shape[0] + test_data_approved.shape[0]\n",
    "), test_data_approved.shape[0] / (\n",
    "    train_data_approved.shape[0] + test_data_approved.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_rejected = data_rejected[msk_rejected]\n",
    "test_data_rejected = data_rejected[~msk_rejected]\n",
    "\n",
    "train_data_rejected.shape[0], test_data_rejected.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_rejected.shape[0] / (\n",
    "    train_data_rejected.shape[0] + test_data_rejected.shape[0]\n",
    "), test_data_rejected.shape[0] / (\n",
    "    train_data_rejected.shape[0] + test_data_rejected.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create behavior object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1. Modelling approved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kt_approved = SpatialModel(\n",
    "    n_dim=1,\n",
    "    min_eigval=1e-9,\n",
    "    n_iterations=200,\n",
    "    scaling_parameter=1.1,\n",
    "    nonparametric=True,\n",
    "    online_learning=False,\n",
    "    loa=True,\n",
    "    limits=np.array([[0], [24]]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_approved = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (\n",
    "            FunctionTransformer(\n",
    "                lambda x: np.array(\n",
    "                    [\n",
    "                        pd.Timestamp(ts).hour\n",
    "                        + pd.Timestamp(ts).minute / 60\n",
    "                        + pd.Timestamp(ts).second / 3600\n",
    "                        for ts in x\n",
    "                    ]\n",
    "                ).reshape(-1, 1),\n",
    "            ),\n",
    "            \"updated_at\",\n",
    "        ),\n",
    "    ),\n",
    "    kt_approved,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_approved.fit(train_data_approved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2. Modelling rejected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kt_rejected = SpatialModel(\n",
    "    n_dim=1,\n",
    "    min_eigval=1e-9,\n",
    "    n_iterations=200,\n",
    "    scaling_parameter=1.1,\n",
    "    nonparametric=True,\n",
    "    online_learning=False,\n",
    "    loa=True,\n",
    "    limits=np.array([[0], [24]]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rejected = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (\n",
    "            FunctionTransformer(\n",
    "                lambda x: np.array(\n",
    "                    [\n",
    "                        pd.Timestamp(ts).hour\n",
    "                        + pd.Timestamp(ts).minute / 60\n",
    "                        + pd.Timestamp(ts).second / 3600\n",
    "                        for ts in x\n",
    "                    ]\n",
    "                ).reshape(-1, 1),\n",
    "            ),\n",
    "            \"updated_at\",\n",
    "        ),\n",
    "    ),\n",
    "    kt_rejected,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rejected.fit(train_data_rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Score samples\n",
    "In a real scenario, whenever we receive a new record we will want to give it a score. This score must be calculated as I will describe below.\n",
    "\n",
    "Kuberspatiotemporal offers 3 alternatives for scoring: 2 of them return a continuos score, like a probability and the other returns a binary score. In this case we will use the simplest approach to compute the probabilistic score because we are only working with continuous data. This scoring approach computes the probability that a given point belongs to the `SpatialModel` - it can be used after activating `SpatialModel.loa`. We can also compute the binary score, that will tell us whether the record was drawn (1) from the model distribution or not (0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 loa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample record\n",
    "record = train_data_approved.iloc[0:1]\n",
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_approved[\"spatialmodel\"].loa = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_approved.score(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 anomaly_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_approved[\"spatialmodel\"].loa = False\n",
    "pipeline_approved[\"spatialmodel\"].score_threshold = pipeline_approved[\n",
    "    \"spatialmodel\"\n",
    "].get_score_threshold(\n",
    "    pipeline_approved[\"columntransformer\"].transform(train_data_approved),\n",
    "    lower_quantile=0,\n",
    "    upper_quantile=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_approved.score(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute model metrics\n",
    "\n",
    "As we collect data and create model, we will want to validate the performance of our models, i.e., to realize how well they are adapted to the data we have and how robust they are to make predictions. Since our data can be classified as approved or rejected requests, we will use this label to compute performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Score samples\n",
    "\n",
    "Let's consider that 0 stands for Rejected transactions and 1 stands for Approved Transactions.\n",
    "\n",
    "In this approach, I'm creating 2 models: 1 to learn the approved requests and another to learn the rejected ones.\n",
    "Here we aim to get not a binary score, but a value between 0 and 1.\n",
    "- **For the approved test set**: we score each sample using both models, the final predicted label will correspond to the maximum score between the two models. Example: for a given approved request on the test set, the approved model will return 0.7 while the rejected model will return 0.1, so we can conclude that this sample corresponds to an true approved.\n",
    "- **For the rejected test set**: we do the same. For a given rejected sample we get 0.8 from the approved model and 0.3 from the rejected which allows us to conclude that this sample is a false approved.\n",
    "\n",
    "Based on this approach we can compute a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(record, pipeline):\n",
    "    pipeline[\"spatialmodel\"].loa = True\n",
    "    return pipeline.score(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_approved = np.argmin(\n",
    "    np.array(\n",
    "        [\n",
    "            [\n",
    "                calculate_score(\n",
    "                    test_data_approved.iloc[[idx]],\n",
    "                    pipeline_approved\n",
    "                ),\n",
    "                calculate_score(\n",
    "                    test_data_approved.iloc[[idx]],\n",
    "                    pipeline_rejected\n",
    "                ),\n",
    "            ]\n",
    "            for idx in range(len(test_data_approved))\n",
    "        ]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_rejected = np.argmax(\n",
    "    np.array(\n",
    "        [\n",
    "            [\n",
    "                calculate_score(test_data_rejected.iloc[[idx]], pipeline_rejected),\n",
    "                calculate_score(test_data_rejected.iloc[[idx]], pipeline_approved),\n",
    "            ]\n",
    "            for idx in range(len(test_data_rejected))\n",
    "        ]\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.concatenate(\n",
    "    (np.zeros(len(pred_test_data_rejected)), np.ones(len(pred_test_data_approved)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate((pred_test_data_rejected, pred_test_data_approved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "f1_score = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(\n",
    "    np.round(\n",
    "        np.array([acc, precision, recall, specificity, f1_score])[:, np.newaxis] * 100,\n",
    "        3,\n",
    "    ),\n",
    "    index=[\"Accuracy\", \"Precision\", \"Recall\", \"Specificity\", \"F1 Score\"],\n",
    "    columns=[\"values\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Different scoring approach \n",
    "With Kubertemporal we can compute a binary score to predict whether a sample was drawn from a given distribution.\n",
    "\n",
    "In this approach, I'm only creating 1 model to learn the approved requests. In the scoring step, I will use this model to score both approved and rejected test samples:\n",
    "- **For the approved test set**: the true approved should get score 1 and the false rejecteds will get score 0;\n",
    "- **For the rejected test set**: the true rejected should get score 0 - the model is assuming that they were not drawn from the approved distribution - and the false approveds will get score 1 - the model says that those rejecteds were drawn from the approved distribution;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_binary_score(record, pipeline, train_data):\n",
    "    pipeline[\"spatialmodel\"].loa = False\n",
    "    pipeline[\"spatialmodel\"].loa = False\n",
    "    pipeline[\"spatialmodel\"].score_threshold = pipeline[\n",
    "        \"spatialmodel\"\n",
    "    ].get_score_threshold(\n",
    "        pipeline[\"columntransformer\"].transform(train_data),\n",
    "        lower_quantile=0,\n",
    "        upper_quantile=0.8,\n",
    "    )\n",
    "    return pipeline.score(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_approved_1 = np.array(\n",
    "    [\n",
    "        [\n",
    "            calculate_binary_score(\n",
    "                test_data_approved.iloc[[idx]], pipeline_approved, train_data_approved,\n",
    "            )\n",
    "        ]\n",
    "        for idx in range(len(test_data_approved))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_rejected_1 = np.array(\n",
    "    [\n",
    "        [\n",
    "            calculate_binary_score(\n",
    "                test_data_rejected.iloc[[idx]], pipeline_approved, train_data_approved,\n",
    "            )\n",
    "        ]\n",
    "        for idx in range(len(test_data_rejected))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate((pred_test_data_rejected_1, pred_test_data_approved_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "f1_score = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(\n",
    "    np.round(\n",
    "        np.array([acc, precision, recall, specificity, f1_score])[:, np.newaxis] * 100,\n",
    "        3,\n",
    "    ),\n",
    "    index=[\"Accuracy\", \"Precision\", \"Recall\", \"Specificity\", \"F1 Score\"],\n",
    "    columns=[\"values\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
