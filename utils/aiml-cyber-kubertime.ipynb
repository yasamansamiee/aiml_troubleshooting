{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubertemporal behavior\n",
    "\n",
    "This chapter demonstrates how a 1D model can be learned over DBFP records.\n",
    "\n",
    "The main goal of this notebook is to train a 1D model per application and per user. `Kuberspatiotemporal` is used to model the time when an authetications was requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from ipywidgets import interact, interactive, Layout, IntSlider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kuberspatiotemporal import CompoundModel, Feature, SpatialModel, KuberModel\n",
    "from kuberspatiotemporal.tools import make_ellipses\n",
    "\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 1. Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    \"/Users/adrianai/Desktop/aiml/aiml-dbfp-anomaly/utils/user_dbfp_mfa.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 # authentications per application per user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For learning behaviors (see respective chapter), it is interesting to sort by applications and see how many datapoints per user and per app (to count combinations, see [here](https://stackoverflow.com/a/25259333)) are in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 10\n",
    "df_per_app_per_user_approved = (\n",
    "    data[data[\"mfa_status\"] == \"approved\"]\n",
    "    .groupby([\"user_id\", \"organization_id\", \"oauth_application_id\"], as_index=False)[\n",
    "        \"created_at\"\n",
    "    ]\n",
    "    .count()\n",
    "    .rename(columns={\"created_at\": \"approved\"})\n",
    "    .sort_values(by=\"approved\", ascending=False)\n",
    ")\n",
    "df_per_app_per_user_rejected = (\n",
    "    data[data[\"mfa_status\"] == \"rejected\"]\n",
    "    .groupby([\"user_id\", \"organization_id\", \"oauth_application_id\"], as_index=False)[\n",
    "        \"created_at\"\n",
    "    ]\n",
    "    .count()\n",
    "    .rename(columns={\"created_at\": \"rejected\"})\n",
    "    .sort_values(by=\"rejected\", ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "df_per_app_per_user = pd.merge(\n",
    "    df_per_app_per_user_approved,\n",
    "    df_per_app_per_user_rejected,\n",
    "    on=[\"user_id\", \"organization_id\", \"oauth_application_id\"],\n",
    ")\n",
    "df_per_app_per_user = df_per_app_per_user.sort_values(\n",
    "    by=[\"approved\", \"rejected\"], ascending=False\n",
    ")\n",
    "\n",
    "\n",
    "def show_dbfp_app_user_counts(i):\n",
    "    display(df_per_app_per_user.iloc[i * step : i * step + step])\n",
    "\n",
    "\n",
    "show_app_user_counts = interactive(\n",
    "    show_dbfp_app_user_counts,\n",
    "    i=IntSlider(\n",
    "        min=0,\n",
    "        max=int(df_per_app_per_user.shape[0] / step - 1),\n",
    "        layout=Layout(width=\"30%\", height=\"100px\"),\n",
    "    ),\n",
    ")\n",
    "show_app_user_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_per_app_per_user[\n",
    "    (df_per_app_per_user[\"approved\"] > 30) & (df_per_app_per_user[\"rejected\"] > 30)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, org, app = (\n",
    "    df_per_app_per_user[\n",
    "        (df_per_app_per_user[\"approved\"] > 30) & (df_per_app_per_user[\"rejected\"] > 30)\n",
    "    ][[\"user_id\", \"organization_id\", \"oauth_application_id\"]]\n",
    "    .head(10)\n",
    "    .values.tolist()[4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user, org, app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Split test and train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_approved = data[\n",
    "    (data[\"mfa_status\"] == \"approved\")\n",
    "    & (data[\"user_id\"] == user)\n",
    "    & (data[\"oauth_application_id\"] == app)\n",
    "    & (data[\"organization_id\"] == org)\n",
    "]\n",
    "data_rejected = data[\n",
    "    (data[\"mfa_status\"] == \"rejected\")\n",
    "    & (data[\"user_id\"] == user)\n",
    "    & (data[\"oauth_application_id\"] == app)\n",
    "    & (data[\"organization_id\"] == org)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk_approved = np.random.rand(len(data_approved)) < 0.7\n",
    "msk_rejected = np.random.rand(len(data_rejected)) < 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_approved = data_approved[msk_approved]\n",
    "test_data_approved = data_approved[~msk_approved]\n",
    "\n",
    "train_data_approved.shape[0], test_data_approved.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_approved.shape[0] / (\n",
    "    train_data_approved.shape[0] + test_data_approved.shape[0]\n",
    "), test_data_approved.shape[0] / (\n",
    "    train_data_approved.shape[0] + test_data_approved.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_rejected = data_rejected[msk_rejected]\n",
    "test_data_rejected = data_rejected[~msk_rejected]\n",
    "\n",
    "train_data_rejected.shape[0], test_data_rejected.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_rejected.shape[0] / (\n",
    "    train_data_rejected.shape[0] + test_data_rejected.shape[0]\n",
    "), test_data_rejected.shape[0] / (\n",
    "    train_data_rejected.shape[0] + test_data_rejected.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create behavior object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kt_approved = SpatialModel(\n",
    "    n_dim=1,\n",
    "    min_eigval=1e-9,\n",
    "    n_iterations=200,\n",
    "    scaling_parameter=1.1,\n",
    "    nonparametric=True,\n",
    "    online_learning=False,\n",
    "    loa=True,\n",
    "    limits=np.array([[0], [24]]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_approved = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (\n",
    "            FunctionTransformer(\n",
    "                lambda x: np.array(\n",
    "                    [\n",
    "                        pd.Timestamp(ts).hour\n",
    "                        + pd.Timestamp(ts).minute / 60\n",
    "                        + pd.Timestamp(ts).second / 3600\n",
    "                        for ts in x\n",
    "                    ]\n",
    "                ).reshape(-1, 1),\n",
    "            ),\n",
    "            \"updated_at\",\n",
    "        ),\n",
    "    ),\n",
    "    kt_approved,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_approved.fit(train_data_approved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kt_rejected = SpatialModel(\n",
    "    n_dim=1,\n",
    "    min_eigval=1e-9,\n",
    "    n_iterations=200,\n",
    "    scaling_parameter=1.1,\n",
    "    nonparametric=True,\n",
    "    online_learning=False,\n",
    "    loa=True,\n",
    "    limits=np.array([[0], [24]]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rejected = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (\n",
    "            FunctionTransformer(\n",
    "                lambda x: np.array(\n",
    "                    [\n",
    "                        pd.Timestamp(ts).hour\n",
    "                        + pd.Timestamp(ts).minute / 60\n",
    "                        + pd.Timestamp(ts).second / 3600\n",
    "                        for ts in x\n",
    "                    ]\n",
    "                ).reshape(-1, 1),\n",
    "            ),\n",
    "            \"updated_at\",\n",
    "        ),\n",
    "    ),\n",
    "    kt_rejected,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rejected.fit(train_data_rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Score samples\n",
    "\n",
    "Let's consider that 0 stands for Rejected transactions and 1 stands for Approved Transactions.\n",
    "\n",
    "In this approach, I'm creating 2 models: 1 to learn the approved requests and another to learn the rejected ones.\n",
    "Here we aim to get not a binary score, but a value between 0 and 1.\n",
    "- **For the approved test set**: we score each sample using both models, the final predicted label will correspond to the maximum score between the two models. Example: for a given approved request on the test set, the approved model will return 0.7 while the rejected model will return 0.1, so we can conclude that this sample corresponds to an true approved.\n",
    "- **For the rejected test set**: we do the same. For a given rejected sample we get 0.8 from the approved model and 0.3 from the rejected which allows us to conclude that this sample is a false approved.\n",
    "\n",
    "Based on this approach we can compute a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(center, model, train_data):\n",
    "    model.loa = False\n",
    "    model.score_threshold = None\n",
    "    model.quantiles = model.get_score_threshold(\n",
    "        train_data, lower_quantile=0, upper_quantile=0.8\n",
    "    )\n",
    "    return model.score(np.array([center]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_rejected = np.argmax(\n",
    "    np.array(\n",
    "        [\n",
    "            [\n",
    "                calculate_score(\n",
    "                    sample,\n",
    "                    kt_rejected,\n",
    "                    pipeline_rejected[\"columntransformer\"].fit_transform(\n",
    "                        train_data_rejected\n",
    "                    ),\n",
    "                ),\n",
    "                calculate_score(\n",
    "                    sample,\n",
    "                    kt_approved,\n",
    "                    pipeline_approved[\"columntransformer\"].fit_transform(\n",
    "                        train_data_approved\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "            for sample in pipeline_rejected[\"columntransformer\"].fit_transform(\n",
    "                train_data_rejected\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_approved = np.argmin(\n",
    "    np.array(\n",
    "        [\n",
    "            [\n",
    "                calculate_score(\n",
    "                    sample,\n",
    "                    kt_approved,\n",
    "                    pipeline_approved[\"columntransformer\"].fit_transform(\n",
    "                        train_data_approved\n",
    "                    ),\n",
    "                ),\n",
    "                calculate_score(\n",
    "                    sample,\n",
    "                    kt_rejected,\n",
    "                    pipeline_rejected[\"columntransformer\"].fit_transform(\n",
    "                        train_data_rejected\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "            for sample in pipeline_approved[\"columntransformer\"].fit_transform(\n",
    "                train_data_approved\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.concatenate(\n",
    "    (np.zeros(len(pred_test_data_rejected)), np.ones(len(pred_test_data_approved)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate((pred_test_data_rejected, pred_test_data_approved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "f1_score = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(\n",
    "    np.round(\n",
    "        np.array([acc, precision, recall, specificity, f1_score])[:, np.newaxis] * 100,\n",
    "        3,\n",
    "    ),\n",
    "    index=[\"Accuracy\", \"Precision\", \"Recall\", \"Specificity\", \"F1 Score\"],\n",
    "    columns=[\"values\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Different scoring approach \n",
    "With Kubertemporal we can compute a binary score to predict whether a sample was drawn from a given distribution.\n",
    "\n",
    "In this approach, I'm only creating 1 model to learn the approved requests. In the scoring step, I will use this model to score both approved and rejected test samples:\n",
    "- **For the approved test set**: the true approved should get score 1 and the false rejecteds will get score 0;\n",
    "- **For the rejected test set**: the true rejected should get score 0 - the model is assuming that they were not drawn from the approved distribution - and the false approveds will get score 1 - the model says that those rejecteds were drawn from the approved distribution;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_binary_score(center, model, train_data):\n",
    "    model.loa = False\n",
    "    model.score_threshold = model.get_score_threshold(\n",
    "        train_data, lower_quantile=0, upper_quantile=0.8\n",
    "    )\n",
    "    return model.score(np.array([center]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_approved_1 = np.array(\n",
    "    [\n",
    "        [\n",
    "            calculate_binary_score(\n",
    "                sample,\n",
    "                kt_approved,\n",
    "                pipeline_approved[\"columntransformer\"].fit_transform(\n",
    "                    train_data_approved\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "        for sample in pipeline_approved[\"columntransformer\"].fit_transform(\n",
    "            train_data_approved\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_data_rejected_1 = np.array(\n",
    "    [\n",
    "        [\n",
    "            calculate_binary_score(\n",
    "                sample,\n",
    "                kt_approved,\n",
    "                pipeline_approved[\"columntransformer\"].fit_transform(\n",
    "                    train_data_approved\n",
    "                ),\n",
    "            )\n",
    "        ]\n",
    "        for sample in pipeline_rejected[\"columntransformer\"].fit_transform(\n",
    "            train_data_rejected\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate((pred_test_data_rejected_1, pred_test_data_approved_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = (tp + tn) / (tn + fp + fn + tp)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "f1_score = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame(\n",
    "    np.round(\n",
    "        np.array([acc, precision, recall, specificity, f1_score])[:, np.newaxis] * 100,\n",
    "        3,\n",
    "    ),\n",
    "    index=[\"Accuracy\", \"Precision\", \"Recall\", \"Specificity\", \"F1 Score\"],\n",
    "    columns=[\"values\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
